% Encoding: UTF-8
@article{du2019multi,
  title={Multi-modal deep learning for landform recognition},
  author={Du, Lin and You, Xiong and Li, Ke and Meng, Liqiu and Cheng, Gong and Xiong, Liyang and Wang, Guangxia},
  journal={ISPRS Journal of Photogrammetry and Remote Sensing},
  volume={158},
  pages={63--75},
  year={2019},
  publisher={Elsevier}
}
@article{hinton2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1106--1114},
  year={2012}
}
@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}
@inproceedings{szegedy2017inception,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}
@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}

@PhdThesis{张秋颖2020,
  author   = {张秋颖},
  school   = {哈尔滨商业大学},
  title    = {基于深度学习的遥感图像场景分类方法研究},
  year     = {2020},
  abstract = {遥感图像场景分类一直是遥感领域重要的研究内容,广泛应用于多个领域,对遥感技术的发展具有重要意义。遥感图像场景分类是根据不同类别遥感图像特征信息的差异性,对遥感图像进行区分。与一般图像相比,遥感图像的纹理和颜色特征信息丰富度更高,同时也存在背景复杂、类型繁多等问题,因此遥感图像场景分类问题存在着一定的挑战。本文提出基于多层次特征结合的遥感图像场景分类方法,将遥感图像的底层特征、中层特征和高层特征结合共同表示遥感场景语义,训练分类器构建遥感图像场景分类模型,不同层次特征互相补充特征信息,以提高分类准确率。首先,提取遥感图像的底层GIST特征,将多颜色空间的GIST特征结合,增强遥感图像的颜色信息;其次,提取遥感图像的中层词袋特征,对遥感图像计算灰度图像、最大值图像、最小值图像和平均值图像,分别对四种图像提取词袋特征,将不同聚类数的最优图像词袋特征组合提高特征表达能力;然后提取遥感图像的高层特征,采用卷积神经网络训练遥感数据集并提取深度特征,将迁移学习策略引入网络训练任务中,节省训练时间,并提高模型性能;最后,将多个层次的图像特征数据归一化到相同范围,以减小不同层次特征数据间的差异,将组合后的特征输入到分类器分类。在UCM Land-Use和WHU-RS19两个遥感数据集上对本文方法进行实验验证,实验结果表明,本文所提的方法与现有其他方法相比分类准确率有所提高,具有较好的优势。},
  keywords = {遥感;场景分类;深度学习;卷积神经网络;多层次特征},
}

@PhdThesis{张艳月2020,
  author   = {张艳月},
  school   = {内蒙古科技大学},
  title    = {基于深度特征融合的遥感图像分类算法研究},
  year     = {2020},
  abstract = {遥感图像分类为当今的社会发展起着不可忽视的作用,因此众多研究者一直在这个领域潜心研究,以提出更先进的理论方法,使其实现更高的价值。机器学习已成为当代研究遥感图像分类的研究方法,同时,该方法也逐渐被广泛应用到其他重大领域中,机器学习的子领域--深度学习,其应用领域日渐广泛。尤其在数字图像处理方面,深度学习取得很大胜利。遥感图像分类即为当下其应用场景之一,但由于人类需求对技术方法要求的不断提高,使得研究方法必须不断更新,因此,深度学习一直是目前处理图像任务的主要钻研方法。本文以卷积神经网络(Convolutional Neural Network,CNN)为基础,针对几种遥感图像数据集设计出具有很好分类效果的模型。以下是论文研究的主要内容:1.针对遥感图像场景分类中由于特征利用不充分导致部分信息缺失进而影响分类精度的问题,提出一种基于密集特征融合的遥感场景分类算法。构建密集网络(Dense Convolutional Network,DenseNet)和扩展DenseNet分别提取图像局部特征图像全局特征,用视觉词袋模型(Bag of Visual Words,BOVW)编码方法进行重组编码,充分表达图像局部深层信息,将两部分网络提取的特征进行线性加权融合并结合softmax分类器进行分类,利用局部和全局的特征互补性,将图形信息充分提取并利用,有利于改善分类精度。2.高光谱遥感影像(Hyperspectral Remote Sensing Image,HSI)数据信息量丰富,光谱分辨率高,具有巨大的应用价值。卷积神经网络已在HSI分类任务中有显著的效果。然而,有限的HSI标记样本使得现有的基于CNN的HSI分类方法通常受到样本规模小和分布类别不平衡的问题的困扰,是目前HSI分类任务中棘手的问题。因此本文设计了一个特殊的CNN体系结构,为解决上述问题找到了方向。在传统网络两侧引入分支结构,设计出多分支结构的CNN融合网络,即宽度更宽和深度更深的卷积神经网络。该网络结构能更有效的提取高光图遥感图像中的有用信息,加之引入L2正则化,使得在对小样本数据集分类时仍然能取得很好的结果,进而提高模型的泛化能力。},
  keywords = {遥感图像分类;多分支融合;深度学习;密集网络},
}

@Article{陈雅琼2019,
  author   = {陈雅琼 and 强振平 and 陈旭 and 刘心怡},
  journal  = {遥感信息},
  title    = {利用微调卷积神经网络的土地利用场景分类},
  year     = {2019},
  number   = {03},
  pages    = {70--77},
  volume   = {34},
  abstract = {针对场景类别之间的相同类内差异性与不同类间相似性所造成的遥感图像场景分类不够精确的问题,提出了将微调(fine-tuning)与卷积神经网络(convolutional neural network,CNN)模型相结合的方法,对土地利用遥感场景图像进行分类。该方法对CNN前层固定,调整分类层,保留了图像的泛性特征;通过卫星影像图获取土地利用场景图块作为训练样本,对训练样本图块进行预处理,然后对在ImageNet数据集上训练得到的AlexNet模型进行fine-tuning,利用得到的CNN模型即可自动提取土地利用遥感图像的图像特征并对其进行分类。为了验证本文方法,对实验区影像进行分割得到测试样本并进行同训练样本一致的预处理,将测试样本的分类结果与随机森林、支持向量机等经典方法的结果进行对比。结果表明,经过fine-tuning的CNN模型在土地利用分类中得到的结果要明显优于其他分类方法。},
  keywords = {微调;卷积神经网络;土地利用;场景分类;AlexNet模型},
}

@Article{业巧林2019,
  author   = {业巧林 and 许等平 and 张冬},
  journal  = {林业工程学报},
  title    = {基于深度学习特征和支持向量机的遥感图像分类},
  year     = {2019},
  number   = {02},
  pages    = {119--125},
  volume   = {4},
  abstract = {随着遥感图像采集技术的迅速发展,传统的遥感图像处理方法已经不能满足当前实际的生产需要。近年来,深度学习模型的流行为遥感图像分类问题的解决提供了新的途径。因此,为了进一步提升遥感图像的分类精度,笔者提出了一种基于深度学习特征和支持向量机(support vector machine,SVM)的遥感图像分类模型。首先,针对深度学习模型需要海量训练数据的特点,运用旋转、剪裁等方法对原始的遥感图像进行数据扩增;然后,将扩增数据按照种类随机地分为训练集和验证集两部分测试集,并使用训练集和验证集训练改进的针对遥感图像分类问题的卷积神经网络(convolutional neural network,CNN)中的参数,进而在训练好的CNN模型上提取第一部分测试集的深度学习特征;最后,使用第一部分测试集的深度特征训练多分类SVM,并对第二部分测试集图像进行分类验证。实验采用NWPU-RESISC45公共数据集对本研究模型精度进行验证,与现有的遥感图像分类方法相比较,实验结果表明,提出模型的总体分类精度有明显提升,从而验证了方法的有效性和实用性。},
  keywords = {深度学习;遥感图像分类;支持向量机;卷积神经网络;分类精度},
}

@Article{张晓男2018,
  author   = {张晓男 and 钟兴 and 朱瑞飞 and 高放 and 张作省 and 鲍松泽 and 李竺强},
  journal  = {光学学报},
  title    = {基于集成卷积神经网络的遥感影像场景分类},
  year     = {2018},
  number   = {11},
  pages    = {350--360},
  volume   = {38},
  abstract = {提出了一种基于集成卷积神经网络(CNN)的遥感影像场景分类算法。通过构建反向传播网络实现了场景图像的复杂度度量;根据图像的复杂度级别,选择CNN对图像进行分类,完成了遥感影像的场景分类。使用所提出的算法对NWPU-RESISC45公开数据集进行了实验验证,取得了89.33%(第一类实验)和92.53%(第二类实验)的分类准确率,平均运行时间为0.41 s。相比于精调训练的VGG-16模型,所提算法的分类准确率分别提升了2.19%和2.17%,预测速率提升了33%,证明了其有效性和实用性。},
  keywords = {遥感;卷积神经网络;图像复杂度;场景分类},
}
@article{li2020deep,
  title={Deep learning-based approach for landform classification from integrated data sources of digital elevation model and imagery},
  author={Li, Sijin and Xiong, Liyang and Tang, Guoan and Strobl, Josef},
  journal={Geomorphology},
  volume={354},
  pages={107045},
  year={2020},
  publisher={Elsevier}
}

@Article{刘凯2016,
  author   = {刘凯 and 汤国安 and 黄骁力 and 蒋圣},
  journal  = {地球信息科学学报},
  title    = {面向地形特征的DEM与影像纹理差异分析},
  year     = {2016},
  number   = {03},
  pages    = {386--395},
  volume   = {18},
  abstract = {纹理分析方法在宏观地形特征分析方面具有较大的优势与潜力,但当前缺少对DEM与影像数据纹理特征差异的系统分析研究。本文采用灰度共生矩阵为纹理量化模型,选取了8个不同地貌单元的样本数据,对DEM和遥感影像2类数据的纹理进行了特征值对比分析、纹理特征稳定性分析、纹理特征组间差异性分析。实验结果表明,在所测试的二阶角矩、对比度、方差、熵4个纹理指标中,DEM和影像的对比度特征值间具有显著的相关性;通过不同地貌样区纹理特征值对比分析发现,DEM数据在地形起伏较大区域纹理特征更为明显,遥感影像数据则受地表覆盖物影响较大;从地形特征的稳定性角度分析,DEM数据在丘陵和山地分析有优势,影像数据则在平原和台地分析表现更好;从地形特征差异性角度分析,DEM数据要优于影像数据。进一步采用光照模拟和坡度数据以增加DEM纹理信息,研究结果表明,DEM派生的2类数据在地形量化差异性方面改进明显,并大大优于影像数据。},
  keywords = {地形特征;纹理分析;DEM;遥感影像;数字地形分析},
}
@article{he2015spatial,
  title={Spatial pyramid pooling in deep convolutional networks for visual recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={9},
  pages={1904--1916},
  year={2015},
  publisher={IEEE}
}
@inproceedings{hu2018squeeze,
  title={Squeeze-and-excitation networks},
  author={Hu, Jie and Shen, Li and Sun, Gang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7132--7141},
  year={2018}
}
@Comment{jabref-meta: databaseType:bibtex;}
